Student Names: Thomas Lux, Randall Pittman
Academic Major: Computer Science
Faculty Mentors: Dr. Bouchard, Dr. Shende
Project Title: Analyzing Techniques for Robotic Localization Mapping

Introduction: This project will analyze several robotic environment mapping techniques.  It will attempt to discern the various advantages and disadvantages to these techniques.  Specifically, it will determine both the efficiency of speed and the accuracy of the map generated by these methods.  The methods that will be analyzed include ultrasonic rangefinding, camera image depth triangulation, and the Microsoft Kinect infrared camera.  These methods for mapping will be tested by developing a simple robot that will autonomously create a 3-dimensional map of its surroundings.


Biographical Sketch:

Randall Pittman:
	Randall is a Computer Science major with a special interest in robotics and mathematics.  Since a young age he has been fascinated by various properties in mathematics, and during high-school developed a keen interest in computer programming.  He enjoys being able to write programs that perform various tasks that help with mathematics or problem solving.  He has an internship over the summer with a software development company testing code in Java.  He has developed an interest in the problem of robotic environment mapping because of the mathematical complexity it involves.  

Thomas Lux:
       Thomas Lux is a rising Sophmore Computer Science major interested in attending graduate school to focus on the departments of machine learning and robotics.  For many years Thomas has been interested in computers and computer science.  Computers have provided Thomas a means to understand and create useful tools for everyday life.  Thomas aspires to continue advancing his knowledge of computers so that in the future he can take part in larger scale projects that truly put the mind to the test.


Project Description: 
	The goal of this project is to compare several different techniques that a robot can use to map its environment.  These techniques will be ultrasonic range finding, camera triangulation, and the Microsoft Kinect infrared camera.  All of these technologies will be implemented through the same basic environment mapping algorithm, so that the most effective method can be found without data error due to the use of different algorithms whose efficiency may vary.  The two main portions of data that will be compared are: first, the ability of the sensor to accurately map an indoor room or building, and second, the time it takes the robot to execute the maneuvers required to build this map.  These will be the main specifications that will determine the comparative capability of these technologies.  Another indirect result of these conclusions will be a measure of how cost effective these devices are for the purposes of environment mapping.  It may help to answer the question of whether it is in fact worth the trouble, for example, to purchase an infrared Kinect camera instead of an ultrasonic sensor, the latter of which may be cheaper and easier to implement.  The question is whether the Kinect shows a significantly greater deal of accuracy than the ultrasonic sensor.  
	The reasons for the importance of the results of this project can be seen particularly in the prevalence of robotic scanning and mapping technology in the modern world.  Computer scientist Michael Montemerlo begins a paper on a specific algorithmic solution to the mapping problem saying, "The ability to simultaneously localize a robot and accurately map its surroundings is considered by many to be a key prerequisite of truly autonomous robots." (1)  In order to navigate any given environment, an autonomous robot (a robot which is not controlled by a user, but by a computer program) must have the ability to understand its surroundings so that it can navigate through obstacles.  Obstacle course navigation, however, will not be a part of this project; the efficient building of an accurate map will be the focus.  Such a map is a prerequisite for any given obstacle navigation algorithm to complete its task.  Author Arturo Gil also describes the importance of efficient map building, "Mobile robots must possess a basic skill: the ability to plan and follow a path through the environment in an optimal way, while avoiding obstacles and computing its location within the map.  In order to solve this problem, mobile robots require the existence of a precise map.  In consequence, map building is an important task for autonomous mobile robots." (5210)  Many developers of navigation algorithms must test for efficiency in an actual environment.  Thus there is demand for simple, cost-effective, and accurate solution to the robotic implementation aspect of such a project.  What then is the best technique to consider putting time and money towards?  
	In a paper discussing some of these techniques, Sebastian Thrun mentions some of the sensors that are to most popular to use when map building, "To acquire a map, robots must possess sensors that enable it to perceive the outside world. Sensors commonly brought to bear for this task include cameras, range finders using sonar, laser, and infrared technology, radar, tactile sensors, compasses, and GPS."  He goes on to say that "all these sensors are subject to errors, often referred to as measurement noise." (2)  Thus it would seem that the data in an analysis of the capabilities of camera sensors, sonar, and an infrared sensor is quite relevant to a computer scientist implementing a mapping algorithm.  Concerning the type of infrared camera, the recent development of Microsoft Kinect infrared camera has raised some interest in the robotic mapping community due to its depth scanning ability.  Thus this project will include testing the implementation of this new technology to compare it with some of the more traditional methods for map building.  
	The plan for obtaining this data can be divided into two segments of work.  First, in order to build a map, a robot is needed.  The equipment that will be used to build this robot will be the Lego Mindstorms product.  We have already begun testing the accuracy of various motors and simple sensors that come with the product, though more advanced sensors will be used later on.  The motors that come with Lego Mindstorms have a relatively high accuracy of spin distance, and this will be useful when the robot is instructed to move an exact amount.  Also, Mindstorms will be used to make a platform capable of holding the three sensors mentioned earlier.  The second phase of work will involve writing the programs and algorithms that can obtain sensor data that is then used to build a map.  Virtually all code used will be written in the Python programming language.  One problem that will need to be overcome in this stage is converting the Microsoft Kinect depth data to a format that can be used in the Python language.  For the camera triangulation, software will be needed that is capable of camparing two images from a camera(s) in different positions and can then compute the distance to various points in the image.  For the ultrasonic sensor, one that is more advanced than the one that is included in Lego Mindstorms will need to be obtained, thus allowing for greater accuracy.  A foreseeable problem may be connecting this new sensor to the Mindstorms local computing device.  

Works Cited:

Montemerlo, Michael, Sebastian Thrun, Daphne Koller, Ben Wegbreit.  FastSLAM: A Factored Solution to the Simultaneous Localization and Mapping Problem.  Association for the Advancement of Artificial Intelligence.  2002. Web.  27 Feb. 2013.  <http://www.aaai.org/Papers/AAAI/2002/AAAI02-089.pdf>

Thrun, Sebastian, Robotic Mapping: A Survey, Pittsburg, PA: Carnegie Mellon University. 2002. Print. 

Gil, Arturo, "Estimation of Visual Maps with a Robot Network Equipped with Vision Sensors."  Sensors (14248220) 10.5 (2010): 5209-5232. Web.  Cited 26 Feb. 2013.


Goals for the project:
      Create and algorithm that builds a map given data points
      Localize position based on current point data and current map
      Test multiple approaches to environmental mapping
      	   Ultrasonic range finding
	   RGB camera depth triangulation
	   Microsoft Kinect multi-point range finding
      Calculate efficiency
      	   Efficiency = (data pionts / time) * spread
	   Spread = total wall space / (average distance between points)
      Log mapping data and create a discrete distribution of recorded points distance from actual points
      Graph the data provided by the distribution of each platform
      Determine the best platform for overall efficiency and accuracy when mapping

Put into words for the papaer

	Efficiency and accuracy are two major factors of robotic mapping.  The goals of this project include testing multiple means of obtaining environmental data, checking all of the data for accuracy, and analyzing this accuracys correspondence with the speed in which was mapped.
	Measuring efficiency will involve a set of recordings and a set of known data.  For the project one algorithm will be used for constructing a virtual map across multiple sensor platforms.   The efficiency rating for each style sensor will be calculated in a simple manner.  Number of data points collected over a certain time interval multiplied by the relative spread of the data points.  Relative spread can be determined by a ratio of given space to average distance between points.  For each type of sensor: ultrasonic range finding, camera image depth-triangulation, and Kinect sensor reading, the number of points obtained in a pre-specified amount of time will be recorded and the corresponding efficiency will be calculated.  Once efficiency is calculated, accuracy is next.
	Accuracy will be measured with regards to map dimensions.  A discrete distribution of each recorded data points distance from true map coordinates will be constructed.  Using this distribution, the mean, the standard deviation, and the relative percent error can be calculated.  Once this data is obtained, the accuracy of each sensor can be cross-examined on a graph.  Accuracy will be a very heavily weighted factor for this project.  All proceeding calculations for localization and path planning will be formed from the current working map.
	Once all data on efficiency and accuracy for each type of sensor has been obtained, each sensor can be compared on a larger scale.  This new perspective will provide relevant information for future researchers looking to construct autonomous mapping robots.

